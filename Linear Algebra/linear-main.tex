\documentclass{article}

\usepackage{preamble}

\title{Linear Algebra Qual Solutions}
\author{Madison Phelps \thanks{Oregon State University, phelpmad@oregonstate.edu}}
\date{\today}

\begin{document}

\maketitle

\thispagestyle{empty}

\break

\tableofcontents

\thispagestyle{empty}

\break

\section{Eigenvectors and Eigenvalues}

\subsection{Problems} 

\begin{problem}{\#3, Jordan Qual Week 1 - Linear Algebra} Suppose $T$ is a linear operator on $V$, a real vector space. Prove that if every nonzero vector in $V$ is an eigenvector of $T$, then $T$ is a multiple of the identity.
\end{problem}

\textbf{Solution:}

Assume dim$(V) = n$. Let $\beta = \{\beta_1, \beta_2, \dots, \beta_n\}$ be a basis for $V$. Then, each $\beta_i\in V$ is an eigenvector of $T$ with eigenvalue $\lambda_i$ for $i = 1, \dots, n$. That is, $T\beta_i = \lambda_i \beta_i$. 

Set $v = \beta_1 + \beta_2 + \cdots + \beta_n\in V$. Then $v$ is an eigenvector of $T$ corresponding to
 eigenvalue $\lambda\in\R$. Observe,
 	\begin{align}
		\lambda \beta_1 + \lambda \beta_2 + \cdots + \lambda \beta_n 
			& = \lambda(\beta_1+\cdots+\beta_n) \label{eqn:jqual-week1-RHS}\\
			& = \lambda v \nonumber \\
			& = Tv \nonumber \\
			& = T(\beta_1+\cdots+\beta_n) \nonumber \\
			& = T\beta_1 + \cdots + T\beta_n \nonumber \\
			& =  \lambda_1 \beta_1 + \cdots + \lambda_n \beta_n .\label{eqn:jqual-week1-LHS}
	\end{align}
If $\lambda = 0$ in line (\ref{eqn:jqual-week1-RHS}), then because $\beta$ is linearly independent, 
$\lambda = \lambda_i = 0$ for all $i = 1,\dots n$. Otherwise, $\lambda$ is non zero and if we subtract
 line  (\ref{eqn:jqual-week1-LHS}) from line (\ref{eqn:jqual-week1-LHS}) we obtain,
 	\[ (\lambda - \lambda_1) \beta_1 + \cdots + (\lambda - \lambda_n) \beta_n = 0.\]
Since $\beta$ is linearly independent, $\lambda = \lambda_1=\cdots = \lambda_n$. Thus, $T$ only has one eigenvalue such that $Tx = \lambda x$ for all $x\in V$ and writing the matrix representation of $T$ with respect to the arbitrary basis $\beta$ is 
	\[ [T]_\beta = \begin{bmatrix} 
				\lambda & 0 & 0 & \dots & 0\\
				0 & \lambda & 0 & \dots & 0\\
				\vdots & \vdots & \vdots & \ddots & 0\\
				0 & 0 & 0 & \dots & \lambda 
				\end{bmatrix}\]
which is a multiple of the $n\times n$ identity matrix. Note that if a nonzero term appeared off of the diagonal, then this would imply that there exists a vector in $V$ that is not an eigenvector, which contradicts our assumption that all vectors in $V$ are eigenvectors. Since $\beta$ was chosen arbitrarily, every matrix representation of $T$ is of the above form. Therefore, $T$ is a multiple of the identity matrix. \\

\hrule 

\textbf{Notes:} Well, this problem boils down to thinking about the eigenvalue associated to each eigenvector. If every eigenvalue is an eigenvector then there is only one true eigenvalue. We show this by taking an arbitrary basis $\beta$ and then each basis element is itself an eigenvector which has a corresponding eigenvalue. We use the fact that $\beta$ is linearly independent to show that all eigenvalues must equal. If we have zero, that's also fine because the sum of the $\lambda_i \beta_i$ is a linear combination equaling zero, which forces each $\lambda_i = 0$. Then, it is still needed to show that $T$ is a multiple of the identity operator, or that $T$ is a multiple of the identity matrix.\\

Another way to look at this is to take two distinct vectors in $V$ (and linearly independent) to show by contradiction that they must be equal because $(\lambda - \lambda_1) v_1 + (\lambda - \lambda_2) v_2 = 0$. Next you can write the matrix representation of $T$ in its Jordan Canonical Form because $T$ only has one eigenvalue, the Jordan Canonical Form could potentially have 1's on the super-diagonal which correspond to generalized eigenvectors. But, these vectors are still in $V$ which implies that there are no 1's on the super-diagonal. We can conclude the same thing that $T$ is a multiple 
of the identity.\\

\hrule \vspace{2pt}
\hrule

\break 

\begin{problem}{\# 3, Jordan Qual Week 1} Let $T$ be a linear operator on a finite dimensional vector space $V$. Prove that
$T$ is a multiple of the identity if and only if $T$ commutes with every other linear operator on $V$. 
\end{problem}

\textbf{Solution:} $\implies$ 

Suppose $T$ is a multiple of the identity operator for some $\lambda$ in the underlying field of $V$. Since the identity commutes with every other linear operator, $T$ commutes with every other linear operator on $V$.


\textbf{Solution:} $\impliedby$ 

Let $\beta$ be an arbitrary basis for $V$ and let $A = [T]_\beta$ be the matrix representation of $T$ with respect to the basis $\beta$. 

Define $E$ to be the $n\times n$ matrix with $E_{ii} = 1$ and all other entries are zero for some $i = 1, \dots, n$. We compute,
	\[ AE = \begin{bmatrix} 0 & \cdots & 0 & A_{1i} & 0 & \cdots & 0\\
					   0 & \cdots & 0 & A_{2i} & 0 & \cdots & 0 \\
					   \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
					    0 & \cdots & 0 & A_{ii} & 0 & \cdots & 0 \\
					     \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
					     0 & \cdots & 0 & A_{ni} & 0 & \cdots & 0
		\end{bmatrix}\]
and 
	\[ EA = \begin{bmatrix} 0 & \cdots & 0 & 0 & \cdots & 0\\
					   \vdots & \vdots & \vdots& \vdots& \vdots & \vdots \\
					   0 & \cdots & 0 & 0& \cdots & 0 \\
					    A_{i1} & A_{i2} & \cdots & A_{ii} & \cdots & A_{in} \\
					    0 & \cdots & 0 & 0&  \cdots & 0 \\
					     \vdots & \vdots & \vdots& \vdots& \vdots & \vdots \\
					     0 & \cdots & 0 & 0 &  \cdots & 0
		\end{bmatrix}\]
In words, multiplying on the left by $E$ results in the matrix whose $i$-th column is the $i$-th column of $A$ and all other entries are 0. Multiplying on the right by $E$ results in the matrix whose $i$-th row is the $i$-th row of $A$. Since $A$ commutes with every matrix, $EA = AE$. Also, $i = 1,\dots, n$ is arbitrary in the above calculation and so, we conclude that for all $i\neq j$, $A_{ij} = 0$. That is, $A$ is a diagonal matrix.
%Then, for each $i = 1, \dots, n$ multiplying $A$ by $E_{ii}$ on the left, i.e., we compute $E_{ii} A$, for which the resulting matrixs selects the $i$-th row of $A$ and every other entry is 0. That is, $a_{ik}\neq 0$ for $k = 1, \dots, n$. Now, multiplying $A$ by $E_{ii}$ on the right selects the $i$-th column of $A$ and every other entry is 0. That is, $a_{ki} \neq 0$ for $k = 1, \dots, n$. Since $A$ commutes with every matrix, we have that $E_{ii} A = A E_{ii}$. So, for all $k\neq i$, $a_{ki} = a_{ik} = 0$. Therefore, $A$ is a diagonal matrix. 

Define $E$ to be the matrix with $E_{11} = 1$ and $E_{1i}$ for some $i = 1,\dots, n$, i.e., 
	\[ E = \begin{bmatrix} 1 & 0 & \cdots & 0 & 1 & 0 & \cdots & 0\\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0\\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0
			\end{bmatrix}\]
Since $A$ is diagonal, the eigenvalues are along the diagonal of the matrix. Suppose that one entry along the diagonal is different. Specifically, suppose the $i$-th entry along the diagonal equals $\lambda_i$ and every other entry equals $\lambda$ such that $\lambda\neq \lambda_i$. We compute,
	\[ EA = \begin{bmatrix} \lambda & 0 & \cdots & 0 & \lambda_i & 0 & \cdots & 0\\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0\\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0
			\end{bmatrix}\]
and 
	\[ AE = \begin{bmatrix} \lambda & 0 & \cdots & 0 & \lambda & 0 & \cdots & 0\\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0\\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0
			\end{bmatrix}\]
In words, because $A$ diagonal, multiplying $E$ by $A$ on the right scales the $i$-th column of $E$ by the $A_{ii}$ entry in $A$. Then, multiplying $E$ by $A$ on the left scales the $i$-th row by the entry $A_{ii}$. Since $A$ commutes with every matrix, $EA=AE$ implies that $\lambda = \lambda_i$ which contradicts our assumption that $\lambda\neq \lambda_i$. Therefore, every entry along the diagonal of $A$ equals $\lambda$, and so, $A$ is a multiple of the identity matrix, where $A$ is the matrix representation of the linear operator $T$ with respect to an arbitrary basis $\beta$.  

\end{document}