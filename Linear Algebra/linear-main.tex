\documentclass{article}

\usepackage{preamble}

\title{Linear Algebra Qual Solutions}
\author{Madison Phelps \thanks{Oregon State University, phelpmad@oregonstate.edu}}
\date{\today}

\begin{document}

\maketitle

\thispagestyle{empty}

\break

\tableofcontents

\thispagestyle{empty}

\break

\section{Eigenvectors and Eigenvalues}

\subsection{Problems} 

\begin{problem}{\#3, Jordan Qual Week 1 - Linear Algebra} Suppose $T$ is a linear operator on $V$, a real vector space. Prove that if every nonzero vector in $V$ is an eigenvector of $T$, then $T$ is a multiple of the identity.
\end{problem}

\textbf{Solution:}

Assume dim$(V) = n$. Let $\beta = \{\beta_1, \beta_2, \dots, \beta_n\}$ be a basis for $V$. Then, each $\beta_i\in V$ is an eigenvector of $T$ with eigenvalue $\lambda_i$ for $i = 1, \dots, n$. That is, $T\beta_i = \lambda_i \beta_i$. 

Set $v = \beta_1 + \beta_2 + \cdots + \beta_n\in V$. Then $v$ is an eigenvector of $T$ corresponding to
 eigenvalue $\lambda\in\R$. Observe,
 	\begin{align}
		\lambda \beta_1 + \lambda \beta_2 + \cdots + \lambda \beta_n 
			& = \lambda(\beta_1+\cdots+\beta_n) \label{eqn:jqual-week1-RHS}\\
			& = \lambda v \nonumber \\
			& = Tv \nonumber \\
			& = T(\beta_1+\cdots+\beta_n) \nonumber \\
			& = T\beta_1 + \cdots + T\beta_n \nonumber \\
			& =  \lambda_1 \beta_1 + \cdots + \lambda_n \beta_n .\label{eqn:jqual-week1-LHS}
	\end{align}
If $\lambda = 0$ in line (\ref{eqn:jqual-week1-RHS}), then because $\beta$ is linearly independent, 
$\lambda = \lambda_i = 0$ for all $i = 1,\dots n$. Otherwise, $\lambda$ is non zero and if we subtract
 line  (\ref{eqn:jqual-week1-LHS}) from line (\ref{eqn:jqual-week1-LHS}) we obtain,
 	\[ (\lambda - \lambda_1) \beta_1 + \cdots + (\lambda - \lambda_n) \beta_n = 0.\]
Since $\beta$ is linearly independent, $\lambda = \lambda_1=\cdots = \lambda_n$. Thus, $T$ only has one eigenvalue such that $Tx = \lambda x$ for all $x\in V$ and writing the matrix representation of $T$ with respect to the arbitrary basis $\beta$ is 
	\[ [T]_\beta = \begin{bmatrix} 
				\lambda & 0  & 0  & \dots & 0 \\
				0 & \lambda & 0 & \dots & 0\\
				\vdots & \vdots & \vdots & \ddots & 0\\
				0 & 0 & 0 & \dots & \lambda 
				\end{bmatrix}\]
which is a multiple of the $n\times n$ identity matrix. Note that if a nonzero term appeared off of the diagonal, then this would imply that there exists a vector in $V$ that is not an eigenvector, which contradicts our assumption that all vectors in $V$ are eigenvectors. Since $\beta$ was chosen arbitrarily, every matrix representation of $T$ is of the above form. Therefore, $T$ is a multiple of the identity matrix. \\

\hrule 

\textbf{Notes:} Well, this problem boils down to thinking about the eigenvalue associated to each eigenvector. If every eigenvalue is an eigenvector then there is only one true eigenvalue. We show this by taking an arbitrary basis $\beta$ and then each basis element is itself an eigenvector which has a corresponding eigenvalue. We use the fact that $\beta$ is linearly independent to show that all eigenvalues must equal. If we have zero, that's also fine because the sum of the $\lambda_i \beta_i$ is a linear combination equaling zero, which forces each $\lambda_i = 0$. Then, it is still needed to show that $T$ is a multiple of the identity operator, or that $T$ is a multiple of the identity matrix.\\

Another way to look at this is to take two distinct vectors in $V$ (and linearly independent) to show by contradiction that they must be equal because $(\lambda - \lambda_1) v_1 + (\lambda - \lambda_2) v_2 = 0$. Next you can write the matrix representation of $T$ in its Jordan Canonical Form because $T$ only has one eigenvalue, the Jordan Canonical Form could potentially have 1's on the super-diagonal which correspond to generalized eigenvectors. But, these vectors are still in $V$ which implies that there are no 1's on the super-diagonal. We can conclude the same thing that $T$ is a multiple 
of the identity.\\

\hrule \vspace{2pt}
\hrule

\break 

\begin{problem}{\# 3, Jordan Qual Week 1} Let $T$ be a linear operator on a finite dimensional vector space $V$. Prove that
$T$ is a multiple of the identity if and only if $T$ commutes with every other linear operator on $V$. 
\end{problem}

\textbf{Solution:} $\implies$ 

Suppose $T$ is a multiple of the identity operator for some $\lambda$ in the underlying field of $V$. Since the identity commutes with every other linear operator, $T$ commutes with every other linear operator on $V$.


\textbf{Solution:} $\impliedby$ 

Let $\beta$ be an arbitrary basis for $V$ and let $A = [T]_\beta$ be the matrix representation of $T$ with respect to the basis $\beta$. 

Define $E$ to be the $n\times n$ matrix with $E_{ii} = 1$ and all other entries are zero for some $i = 1, \dots, n$. We compute,
	\[ AE = \begin{bmatrix} 0 & \cdots & 0 & A_{1i} & 0 & \cdots & 0\\
					   0 & \cdots & 0 & A_{2i} & 0 & \cdots & 0 \\
					   \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
					    0 & \cdots & 0 & A_{ii} & 0 & \cdots & 0 \\
					     \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
					     0 & \cdots & 0 & A_{ni} & 0 & \cdots & 0
		\end{bmatrix}\]
and 
	\[ EA = \begin{bmatrix} 0 & \cdots & 0 & 0 & \cdots & 0\\
					   \vdots & \vdots & \vdots& \vdots& \vdots & \vdots \\
					   0 & \cdots & 0 & 0& \cdots & 0 \\
					    A_{i1} & A_{i2} & \cdots & A_{ii} & \cdots & A_{in} \\
					    0 & \cdots & 0 & 0&  \cdots & 0 \\
					     \vdots & \vdots & \vdots& \vdots& \vdots & \vdots \\
					     0 & \cdots & 0 & 0 &  \cdots & 0
		\end{bmatrix}\]
In words, multiplying on the left by $E$ results in the matrix whose $i$-th column is the $i$-th column of $A$ and all other entries are 0. Multiplying on the right by $E$ results in the matrix whose $i$-th row is the $i$-th row of $A$. Since $A$ commutes with every matrix, $EA = AE$. Also, $i = 1,\dots, n$ is arbitrary in the above calculation and so, we conclude that for all $i\neq j$, $A_{ij} = 0$. That is, $A$ is a diagonal matrix.
%Then, for each $i = 1, \dots, n$ multiplying $A$ by $E_{ii}$ on the left, i.e., we compute $E_{ii} A$, for which the resulting matrixs selects the $i$-th row of $A$ and every other entry is 0. That is, $a_{ik}\neq 0$ for $k = 1, \dots, n$. Now, multiplying $A$ by $E_{ii}$ on the right selects the $i$-th column of $A$ and every other entry is 0. That is, $a_{ki} \neq 0$ for $k = 1, \dots, n$. Since $A$ commutes with every matrix, we have that $E_{ii} A = A E_{ii}$. So, for all $k\neq i$, $a_{ki} = a_{ik} = 0$. Therefore, $A$ is a diagonal matrix. 

Define $E$ to be the matrix with $E_{11} = 1$ and $E_{1i}$ for some $i = 1,\dots, n$, i.e., 
	\[ E = \begin{bmatrix} 1 & 0 & \cdots & 0 & 1 & 0 & \cdots & 0\\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0\\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0
			\end{bmatrix}\]
Since $A$ is diagonal, the eigenvalues are along the diagonal of the matrix. Suppose that one entry along the diagonal is different. Specifically, suppose the $i$-th entry along the diagonal equals $\lambda_i$ and every other entry equals $\lambda$ such that $\lambda\neq \lambda_i$. We compute,
	\[ EA = \begin{bmatrix} \lambda & 0 & \cdots & 0 & \lambda_i & 0 & \cdots & 0\\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0\\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0
			\end{bmatrix}\]
and 
	\[ AE = \begin{bmatrix} \lambda & 0 & \cdots & 0 & \lambda & 0 & \cdots & 0\\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0\\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0
			\end{bmatrix}\]
In words, because $A$ diagonal, multiplying $E$ by $A$ on the right scales the $i$-th column of $E$ by the $A_{ii}$ entry in $A$. Then, multiplying $E$ by $A$ on the left scales the $i$-th row by the entry $A_{ii}$. Since $A$ commutes with every matrix, $EA=AE$ implies that $\lambda = \lambda_i$ which contradicts our assumption that $\lambda\neq \lambda_i$. Therefore, every entry along the diagonal of $A$ equals $\lambda$, and so, $A$ is a multiple of the identity matrix, where $A$ is the matrix representation of the linear operator $T$ with respect to an arbitrary basis $\beta$. \\

\hrule 

\textbf{Notes:} Well, again. Really good problem to look at. Yes, it was not a qual problem but it also points out a tool that we can use whenever we see ``for every" something about the problem does "BLANK" and how we can sometimes use that to our advantage. Also, this problem is a good way to fuss with notation. Of course on the quals you should really label the pictures but at least now you have some notation to fall back on. For example, 
	\begin{itemize}
		\item we define $E$ to be a matrix
		\item entries in $E$ are given by $E_{ij}$ to denote the entry in the $i$-th row and $j$-th 
			column of $E$, i.e., the $(i,j)$-th entry
		\item columns of $E$ are denoted by $E_j$
		\item while rows of $E$ are denoted by $E^i$
	\end{itemize}
Just stick with the above notation and redefine $E$ if you are using the matrix in different ways. They will be okay with 
redefining the matrix because they are mathematicians. 

Matrix multiplication has always fucked me up. So, with this one you really had to think about what the matrix $E$ is doing in the first part vs in the second part. Actually, we are talking about the same thing just in the first part $E$ is the diagonal matrix whereas in the second part, $A$ is the diagonal matrix. Think about how a row vector times a matrix is a row vector and a matrix times a column vector is a column vector. So,

	\[ \begin{bmatrix}\begin{array}{c!{\vline width 2pt} c!{\vline width 2pt} c !{\vline width 2pt} c !{\vline width 2pt} c}
				\; & \; & \; & \; & \; \\ 
				\; & \; & \; & \; & \;\\
				\; & \; & \; & \;& \; \\
				\; & \; & \; & \;& \; \\
			\end{array}
		\end{bmatrix}
		\begin{bmatrix}
		* \\
		* \\
		* \\
		* 
		\end{bmatrix} 
		= * 
		\begin{bmatrix}
		\begin{array} {c!{\vline width 2pt} l}
		 \; &\\
		 \; &\\
		 \; &\\
		 \; &
		\end{array}
		\end{bmatrix} 
		 + * 
		\begin{bmatrix}
		\begin{array} {c!{\vline width 2pt} l}
		 \; &\\
		 \; &\\
		 \; &\\
		 \; &
		\end{array}
		\end{bmatrix} 
		 + *
		\begin{bmatrix}
		\begin{array} {c!{\vline width 2pt} l}
		 \; &\\
		 \; &\\
		 \; &\\
		 \; &
		\end{array}
		\end{bmatrix} 
		 + * 
		\begin{bmatrix}
		\begin{array} {c!{\vline width 2pt} l}
		 \; &\\
		 \; &\\
		 \; &\\
		 \; &
		\end{array}
		\end{bmatrix} 
	\]
	
In words, multiplying a matrix by a column vector means that each entry of the column vector scales the corresponding column in the matrix by that entry. That sounds weird, the first entry of the column vector scales the first column of $A$, the second entry in the column vector scales the second column of $A$, and continues. We can think about the column vector rotating 90 degrees and imposing into the matrix $A$ and then collapsing horizontally to create the resulting column vector $Ax = b$.

On the other side, if we multiply $A$ by a row vector, we are multiplying $A$ on the left by the row vector and similarly we have 
\begin{align*}
	\begin{bmatrix} 
		* & * & * & * 
	\end{bmatrix}
	\begin{bmatrix}
	\begin{array}{c c c c c c}
	& & & & & \\ \Xhline{2pt}
	& & & & & \\ \Xhline{2pt}
	& & & & & \\ \Xhline{2pt}
	& & & & &\\ \Xhline{2pt}
	& & & & &
	\end{array}
	\end{bmatrix}  = \;\;\;\;\;&* \begin{bmatrix}\begin{array}{c c c c c c}
	& & & & & \\ \Xhline{2pt}
	& & & & & \\ 
	\end{array}
	\end{bmatrix} \\
 + \;\;& 
	* \begin{bmatrix}\begin{array}{c c c c c c}
	& & & & & \\ \Xhline{2pt}
	& & & & & \\ 
	\end{array}
	\end{bmatrix} \\\\
 + \;\;& 
	* \begin{bmatrix}\begin{array}{c c c c c c}
	& & & & & \\ \Xhline{2pt}
	& & & & & \\ 
	\end{array}
	\end{bmatrix} \\\\
+\;\; & 
	* \begin{bmatrix}\begin{array}{c c c c c c}
	& & & & & \\ \Xhline{2pt}
	& & & & & \\ 
	\end{array}
	\end{bmatrix}\\
	\cline{2-2} 
	= &\;\;\; \text{the row vector}
\end{align*}
So, what this does is that the row vector is rotating 90 degrees and then imposing itself on the rows of $A$ to scale each row by the entries in the row vector. Then, we add together by collapsing downwards (vertically) to compute the resulting row vector of $x^TA = b^T$.

So, when we are working with diagonal matrices we do the same thing because all entries off the diagonal are 0 the resulting computation of $A$ times a diagonal matrix on the left will scale the rows by each diagonal entry whereas if we multiply $A$ by the diagonal matrix on the right we will scale each column of $A$ by the corresponding diagonal entry. 

Super super cool stuff.\\

\hrule\vspace{2pt}
\hrule
\break 

\begin{problem}{\#2, Summer 2021} Let $A$ be an $n\times n$ real matrix. Suppose $\tr{AX} = 0$ for any $n\times n$ real matrix $X$ with $\tr{X} = 0$. Prove that $A = \lambda I_{n\times n}$ for some $\lambda \in \R$.
\end{problem}

\textbf{Solution:} 

Let $i = 2,\dots, n$ be arbitrary and define $E$ to be the matrix where $E_{11} = 1$ and $E_{ii} = -1$ and otherwise all entries of $E$ are equal to 0. Then, 
	\[ \tr{E} = \sum_{k=1}^n E_{kk} = 1 + 0 + \cdots + 0 + (-1) + 0 + \cdots 0 = 0.\]
We compute,
	\begin{align*} 
		AE & =  
		\begin{bmatrix}
			& \\\\
			& \\\\
			\;\;A_1 & A_2 & \cdots & A_i & \cdots & A_n
			& \\\\\\
			&
		\end{bmatrix}	 
		\begin{bmatrix}
			\begin{array}{cccccccc}
 				1 &   &   &   &   &   &   &   \\
 				  & 0 &   &   &   & \mathbf 0  &   &   \\
 				  &   & \ddots &   &   &   &   &   \\
				  &   &   & 0 &   &   &   &   \\
 				  &   &   &   & -1 &   &   &   \\
 				  &   &   &   &   & 0 &   &   \\
 				  &   &   \mathbf 0&   &   &   & \ddots &   \\
 				  &   &   &   &   &   &   & 0 \\
			\end{array}
		   \end{bmatrix}\\
		   & \quad \quad \quad = \begin{bmatrix} 
		   	& \\\\
			& \\
		   	A_1 & \mathbf 0 & \cdots & \mathbf 0  & -A_i & \mathbf 0 & \cdots & \mathbf 0\\
			& \\\\
			&
			\end{bmatrix}\\
		 & \quad \quad \quad = \begin{bmatrix} 
		 	\begin{array}{cccccccc}
 				A_{11} &   &   &   &   &   &   &   \\
 				  & 0 &   &   &   & \Box  &   &   \\
 				  &   & \ddots &   &   &   &   &   \\
				  &   &   & 0 &   &   &   &   \\
 				  &   &   &   & -A_{ii} &   &   &   \\
 				  &   &   &   &   & 0 &   &   \\
 				  &   &   \Box &   &   &   & \ddots &   \\
 				  &   &   &   &   &   &   & 0 \\
			\end{array}
		 \end{bmatrix}
	\end{align*}
where the boxes denote the entries above and below the diagonal and $A_k$ denotes the $k$-th column of $A$. Since $\tr{E} = 0$, $\tr{AE} = 0$ which implies that $A_{11} - A_{ii} = 0$, i.e., $A_{11} = A_{ii}$ for all $i = 2, \dots, n$. Let $\lambda \in \R$. Then, the entries along the diagonal of $A$ are all equal to $\lambda$. 
	
Now, let $E$ be the matrix with $E_{kk} = 0$ for all $k = 1, \dots, n$ and $E_{ij} = 1$ for some $i\neq j$ and all other entries are equal to 0. In other words, the $j$-th column of $E$ is a column vector of all zeros except in the $i$-th row for $i\neq j$. We compute,
	\begin{align*} 
		AE & =  
		\begin{bmatrix}
			& \\\\
			& \\\\
			A_1 & \cdots & A_i & \cdots & A_n
			& \\\\\\
			&
		\end{bmatrix}
		\begin{bmatrix}
			& \\\\
			& \\\\
			\;\;\mathbf 0 & \cdots & \mathbf 0& E_j & \mathbf 0 \cdots & \mathbf 0
			& \\\\\\
			&
		\end{bmatrix}\\
		& \quad =  \begin{bmatrix}
			& \\\\
			& \\\\
			\;\;\mathbf 0 & \cdots & \mathbf 0& AE_j & \mathbf 0 \cdots & \mathbf 0
			& \\\\\\
			&
		\end{bmatrix}
	\end{align*}
and compute $AE_j$ (the matrix $A$ times the column vector $E_j$) 
	\[AE_j = 0\cdot A_1 + \cdots + 0 \cdot A_{i-1} + 1\cdot A_i + 0\cdot A_{i+1} + \cdots + 0\cdot A_n = A_i\]
which means that the $j$-th column of $AE$ equals the $i$-th column of $A$, since $E_j$ is the column vector with 1 in the $i$-th slot for $i\neq j$. That is, 
	\[A_{1i} = (AE)_{1j}, \dots, A_{ij} = (AE)_{jj}, \dots, \text{ and } A_{1n} = (AE)_{nj}.\]
	 By the assumption that $\tr{AE} = 0$ for any matrix with $\tr{E} = 0$, we conclude that $A_{ij} = 0$. Since $i$ and $j$ with $i\neq j$ were arbitrary in the above calculation, $A_{ij} = 0$ for all $i\neq j$. 

Therefore, $A = \lambda I_{n\times n}$ for some $\lambda \in \R$.\\

\hrule

\textbf{Notes:} Welp. I did that. Time to take a break.\\

\hrule \vspace{2pt}
\hrule


\break

\section{Determinants}

\subsection{Problems}

\begin{problem}{\#4, Spring 2022} Let $a$ and $b$ be $n\times 1$ column vectors with entries from a field $F$. 
	\begin{enumerate}
		\item[(a)] Verify the $n = 3$ case of the identity $\det{I + ab^t} = 1 + a^tb$.
		\item[(b)] Suppose $A$ is an $n\times n$ nonsingular matrix. Assuming the identity in (a), prove that 
				\[\det{A + ab^t} = (1+ b^t A^{-1} a )\det{A}.\]
		\item[(c)] Suppose $M = (m_{ij})$ a an $n\times n$ matrix such that 
				\[ m_{ij} = \begin{cases}
							1 + d_i & \text{ if } i = j,\\
							1 & \text{ if } i \neq j,
						\end{cases}
						\quad \text{ where } d_i \neq 0 \text{ for all } i.\]
			Prove that 
				\[\det{M} = \prod_{i=1}^n d_i \left( 1 + \sum_{i=1}^n \frac{1}{d_i} \right).\]
		\end{enumerate}
\end{problem}

\textbf{Solution:} (a)

USE CRAMER's RULE and everything cancels. If you have time, type it up. If not, just remember that for a three by three matrix you adjoin the first two columns of $A$ to the end of $A$ and multiple the diagonal entries from top to bottom then, starting at the entry in the first row second column and multiple these entries together, repeat with the last entry in the first column. Do the same but in reverse by multiplying diagonally from the bottom to the top. Add the things you multiplied downwards and subtract the things you multiplied upwards. This will be the determinant. 

\textbf{Solution:} (b)

We compute
	\begin{align*}
		\det{A + ab^t} & = \det{A + (AA^{-1}) ab^t} & [AA^{-1} = I]\\
				      & = \det{A(I+A^{-1} ab^t)}\\
				      & = \det{A}\det{I+ (A^{-1}a)b^t} & [\det{CD} = \det{C}\det{D}]\\
				      & = \det{A}\det{\left(I+ (A^{-1}a)b^t\right)^t} & [\det{C^t} = \det{C}]\\
				      & = \det{A}\det{I + b (A^{-1}a)^t}\\
				      & = \left(1 + b^t(A^{-1} a)\right)\det{A}. & [\text{Apply part (a) with } ``a":= b, ``b":=A^{-1}a]
	\end{align*}

\textbf{Solution:} (c) 

Let $a$ be the column vector with $a_i = 1$ for all $i=1,\dots, n$. Then, $aa^t$ is the $n\times n$ matrix with every entry equal to 1. Let $A$ be the diagonal matrix with entries $A_{ii} = d_i$ for all $i=1,\dots, n$. Because $d_i\neq0$, $A$ is an nonsingular matrix whose inverse, $A^{-1}$, is also diagonal with $(A^{-1})_{ii} = \frac{1}{d_i}$ for all $i=1,\dots, n$. Observe that we can write $M = A + aa^t$. By part (b) have that,
	\[\det{M} = \det{A + aa^t} = (1 + a^t A^{-1} a)\det{A}.\]
So, we compute $\det{A}$ and $a^t A^{-1} a$. Since $A$ is diagonal, the determinant is the product of the eigenvalues (which lie on the diagonal) meaning that,
	 \[\det{A} = \prod_{i=1}^n A_{ii} = \prod_{i=1}^n d_i,\]
and 
	\[a^t A^{-1} a = \begin{bmatrix} 
					1 & 1 & \cdots & 1
				\end{bmatrix}
				\begin{bmatrix}
					\begin{array}{ccccc}
 						\nicefrac{1}{d_1} &    &  & \mathbf 0   &   \\
 						  & \nicefrac{1}{d_2} &   &   &   \\
 						  &   & \ddots &   &   \\
 						  &   &   & \ddots &   \\
 						  &  \mathbf 0 &  &   & \nicefrac{1}{d_n} 
					\end{array}
				\end{bmatrix}
				\begin{bmatrix} 
					1 \\ 1 \\ \vdots \\ 1
				\end{bmatrix}
				=
				 \begin{bmatrix} 
					1 & 1 & \cdots & 1
				\end{bmatrix}
				\begin{bmatrix} 
					\nicefrac{1}{d_1} \\ \nicefrac{1}{d_1} \\ \vdots \\ \nicefrac{1}{d_1}
				\end{bmatrix}
				=
				\sum_{i=1}^n \nicefrac{1}{d_i}.
				\] 
Altogether, the above computations result in 
	\[\det{M} = \det{A + aa^t} = (1 + a^t A^{-1} a)\det{A} = \prod_{i=1}^n d_i \left( 1 + \sum_{i=1}^n \frac{1}{d_i} \right).\]
	
\hrule 

\textbf{Notes:} This problem is really about looking at it and seeing how the parts can fit together. How you can use the parts to construct a way to solve the last part. Like Tyler keeps on telling me, these problems with multiple parts are really connected and I should think about how to use them to my advantage. This is getting better. \\

Also, throughout thinking about this problem I drew pictures to help me visualize what in the hell these matrices are doing because on one side of things, they are matrices and on the other, they are numbers. So, I tried using row and column blocks with a matrix to see how I can use the equality in part (a) to help me see what I needed to do to show the equality in part (b). They really messed me up with using $a$ and $b$ because in the second part, the roles are switched... and of course, this makes it harder because its not staring you in the face on ``what to choose" to be ``clever" and demolish the test like the demagorgans. God, I love stranger things. Anyways, what did you learn? You learned that drawing pictures is helpful. You learned to rely on the things you knew and also, I always forget what to add and subtract in Cramer's rule. But, at the end of the day you know what you want. So, whatever pops out should be that, right? So, if you're getting a negative when you should be getting a positive, then you should probably reverse the roles. Also, your scratch work for part (b) was all over the place. Yet, one really good thing that you did is that you didn't completely scratch your idea when something didn't instantly work. Just like he says, try and figure out what when wrong and how you can fix the problem to find the solution. I was choosing the wrong $a$ and $b$ which led to working with $(A^{-1})^t$ which we know nothing about. So, you're going on the right track, but your using the wrong thing to get you there. Take a moment, think about what works and what doesn't then think about how you can fix these things to get where you want.\\

Good job.\\

\hrule\vspace{2pt}
\hrule

\break

\section{Orthogonal Projections} 

\subsection{Problems}




\end{document}