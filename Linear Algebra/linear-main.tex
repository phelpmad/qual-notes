\documentclass{article}

\usepackage{preamble}

\title{Linear Algebra Qual Solutions}
\author{Madison Phelps \thanks{Oregon State University, phelpmad@oregonstate.edu}}
\date{\today}

\begin{document}

\maketitle

\thispagestyle{empty}

\break

\tableofcontents

\thispagestyle{empty}

\break

\section{Eigenvectors and Eigenvalues}

\subsection{Problems} 

\begin{problem}{\#3, Jordan Qual Week 1 - Linear Algebra} Suppose $T$ is a linear operator on $V$, a real vector space. Prove that if every nonzero vector in $V$ is an eigenvector of $T$, then $T$ is a multiple of the identity.
\end{problem}

\textbf{Solution:}

Assume dim$(V) = n$. Let $\beta = \{\beta_1, \beta_2, \dots, \beta_n\}$ be a basis for $V$. Then, each $\beta_i\in V$ is an eigenvector of $T$ with eigenvalue $\lambda_i$ for $i = 1, \dots, n$. That is, $T\beta_i = \lambda_i \beta_i$. 

Set $v = \beta_1 + \beta_2 + \cdots + \beta_n\in V$. Then $v$ is an eigenvector of $T$ corresponding to
 eigenvalue $\lambda\in\R$. Observe,
 	\begin{align}
		\lambda \beta_1 + \lambda \beta_2 + \cdots + \lambda \beta_n 
			& = \lambda(\beta_1+\cdots+\beta_n) \label{eqn:jqual-week1-RHS}\\
			& = \lambda v \nonumber \\
			& = Tv \nonumber \\
			& = T(\beta_1+\cdots+\beta_n) \nonumber \\
			& = T\beta_1 + \cdots + T\beta_n \nonumber \\
			& =  \lambda_1 \beta_1 + \cdots + \lambda_n \beta_n .\label{eqn:jqual-week1-LHS}
	\end{align}
If $\lambda = 0$ in line (\ref{eqn:jqual-week1-RHS}), then because $\beta$ is linearly independent, 
$\lambda = \lambda_i = 0$ for all $i = 1,\dots n$. Otherwise, $\lambda$ is non zero and if we subtract
 line  (\ref{eqn:jqual-week1-LHS}) from line (\ref{eqn:jqual-week1-LHS}) we obtain,
 	\[ (\lambda - \lambda_1) \beta_1 + \cdots + (\lambda - \lambda_n) \beta_n = 0.\]
Since $\beta$ is linearly independent, $\lambda = \lambda_1=\cdots = \lambda_n$. Thus, $T$ only has one eigenvalue such that $Tx = \lambda x$ for all $x\in V$ and writing the matrix representation of $T$ with respect to the arbitrary basis $\beta$ is 
	\[ [T]_\beta = \begin{bmatrix} 
				\lambda & 0 & 0 & \dots & 0\\
				0 & \lambda & 0 & \dots & 0\\
				\vdots & \vdots & \vdots & \ddots & 0\\
				0 & 0 & 0 & \dots & \lambda 
				\end{bmatrix}\]
which is a multiple of the $n\times n$ identity matrix. Note that if a nonzero term appeared off of the diagonal, then this would imply that there exists a vector in $V$ that is not an eigenvector, which contradicts our assumption that all vectors in $V$ are eigenvectors. Since $\beta$ was chosen arbitrarily, every matrix representation of $T$ is of the above form. Therefore, $T$ is a multiple of the identity matrix. \\

\hrule 

\textbf{Notes:} Well, this problem boils down to thinking about the eigenvalue associated to each eigenvector. If every eigenvalue is an eigenvector then there is only one true eigenvalue. We show this by taking an arbitrary basis $\beta$ and then each basis element is itself an eigenvector which has a corresponding eigenvalue. We use the fact that $\beta$ is linearly independent to show that all eigenvalues must equal. If we have zero, that's also fine because the sum of the $\lambda_i \beta_i$ is a linear combination equaling zero, which forces each $\lambda_i = 0$. Then, it is still needed to show that $T$ is a multiple of the identity operator, or that $T$ is a multiple of the identity matrix.\\

Another way to look at this is to take two distinct vectors in $V$ (and linearly independent) to show by contradiction that they must be equal because $(\lambda - \lambda_1) v_1 + (\lambda - \lambda_2) v_2 = 0$. Next you can write the matrix representation of $T$ in its Jordan Canonical Form because $T$ only has one eigenvalue, the Jordan Canonical Form could potentially have 1's on the super-diagonal which correspond to generalized eigenvectors. But, these vectors are still in $V$ which implies that there are no 1's on the super-diagonal. We can conclude the same thing that $T$ is a multiple 
of the identity.\\

\hrule \vspace{2pt}
\hrule

\break 

\begin{problem}{\# 3, Jordan Qual Week 1} Let $T$ be a linear operator on a finite dimensional vector space $V$. Prove that
$T$ is a multiple of the identity if and only if $T$ commutes with every other linear operator on $V$. 
\end{problem}

\textbf{Solution:} $\implies$ 

Suppose $T$ is a multiple of the identity operator for some $\lambda$ in the underlying field of $V$. Since the identity commutes with every other linear operator, $T$ commutes with every other linear operator on $V$.


\textbf{Solution:} $\impliedby$ 

Let $\beta$ be an arbitrary basis for $V$ and let $A = [T]_\beta$ be the matrix representation of $T$ with respect to the basis $\beta$. 

Define $E$ to be the $n\times n$ matrix with $E_{ii} = 1$ and all other entries are zero for some $i = 1, \dots, n$. We compute,
	\[ AE = \begin{bmatrix} 0 & \cdots & 0 & A_{1i} & 0 & \cdots & 0\\
					   0 & \cdots & 0 & A_{2i} & 0 & \cdots & 0 \\
					   \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
					    0 & \cdots & 0 & A_{ii} & 0 & \cdots & 0 \\
					     \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
					     0 & \cdots & 0 & A_{ni} & 0 & \cdots & 0
		\end{bmatrix}\]
and 
	\[ EA = \begin{bmatrix} 0 & \cdots & 0 & 0 & \cdots & 0\\
					   \vdots & \vdots & \vdots& \vdots& \vdots & \vdots \\
					   0 & \cdots & 0 & 0& \cdots & 0 \\
					    A_{i1} & A_{i2} & \cdots & A_{ii} & \cdots & A_{in} \\
					    0 & \cdots & 0 & 0&  \cdots & 0 \\
					     \vdots & \vdots & \vdots& \vdots& \vdots & \vdots \\
					     0 & \cdots & 0 & 0 &  \cdots & 0
		\end{bmatrix}\]
In words, multiplying on the left by $E$ results in the matrix whose $i$-th column is the $i$-th column of $A$ and all other entries are 0. Multiplying on the right by $E$ results in the matrix whose $i$-th row is the $i$-th row of $A$. Since $A$ commutes with every matrix, $EA = AE$. Also, $i = 1,\dots, n$ is arbitrary in the above calculation and so, we conclude that for all $i\neq j$, $A_{ij} = 0$. That is, $A$ is a diagonal matrix.
%Then, for each $i = 1, \dots, n$ multiplying $A$ by $E_{ii}$ on the left, i.e., we compute $E_{ii} A$, for which the resulting matrixs selects the $i$-th row of $A$ and every other entry is 0. That is, $a_{ik}\neq 0$ for $k = 1, \dots, n$. Now, multiplying $A$ by $E_{ii}$ on the right selects the $i$-th column of $A$ and every other entry is 0. That is, $a_{ki} \neq 0$ for $k = 1, \dots, n$. Since $A$ commutes with every matrix, we have that $E_{ii} A = A E_{ii}$. So, for all $k\neq i$, $a_{ki} = a_{ik} = 0$. Therefore, $A$ is a diagonal matrix. 

Define $E$ to be the matrix with $E_{11} = 1$ and $E_{1i}$ for some $i = 1,\dots, n$, i.e., 
	\[ E = \begin{bmatrix} 1 & 0 & \cdots & 0 & 1 & 0 & \cdots & 0\\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0\\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0
			\end{bmatrix}\]
Since $A$ is diagonal, the eigenvalues are along the diagonal of the matrix. Suppose that one entry along the diagonal is different. Specifically, suppose the $i$-th entry along the diagonal equals $\lambda_i$ and every other entry equals $\lambda$ such that $\lambda\neq \lambda_i$. We compute,
	\[ EA = \begin{bmatrix} \lambda & 0 & \cdots & 0 & \lambda_i & 0 & \cdots & 0\\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0\\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0
			\end{bmatrix}\]
and 
	\[ AE = \begin{bmatrix} \lambda & 0 & \cdots & 0 & \lambda & 0 & \cdots & 0\\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0\\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			\vdots & \vdots & \vdots & \vdots& \vdots& \vdots & \vdots & \vdots \\
			0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0
			\end{bmatrix}\]
In words, because $A$ diagonal, multiplying $E$ by $A$ on the right scales the $i$-th column of $E$ by the $A_{ii}$ entry in $A$. Then, multiplying $E$ by $A$ on the left scales the $i$-th row by the entry $A_{ii}$. Since $A$ commutes with every matrix, $EA=AE$ implies that $\lambda = \lambda_i$ which contradicts our assumption that $\lambda\neq \lambda_i$. Therefore, every entry along the diagonal of $A$ equals $\lambda$, and so, $A$ is a multiple of the identity matrix, where $A$ is the matrix representation of the linear operator $T$ with respect to an arbitrary basis $\beta$. \\

\hrule 

\textbf{Notes:} Well, again. Really good problem to look at. Yes, it was not a qual problem but it also points out a tool that we can use whenever we see ``for every" something about the problem does "BLANK" and how we can sometimes use that to our advantage. Also, this problem is a good way to fuss with notation. Of course on the quals you should really label the pictures but at least now you have some notation to fall back on. For example, 
	\begin{itemize}
		\item we define $E$ to be a matrix
		\item entries in $E$ are given by $E_{ij}$ to denote the entry in the $i$-th row and $j$-th 
			column of $E$, i.e., the $(i,j)$-th entry
		\item columns of $E$ are denoted by $E_j$
		\item while rows of $E$ are denoted by $E^i$
	\end{itemize}
Just stick with the above notation and redefine $E$ if you are using the matrix in different ways. They will be okay with 
redefining the matrix because they are mathematicians. 

Matrix multiplication has always fucked me up. So, with this one you really had to think about what the matrix $E$ is doing in the first part vs in the second part. Actually, we are talking about the same thing just in the first part $E$ is the diagonal matrix whereas in the second part, $A$ is the diagonal matrix. Think about how a row vector times a matrix is a row vector and a matrix times a column vector is a column vector. So,

	\[ \begin{bmatrix}\begin{array}{c!{\vline width 2pt} c!{\vline width 2pt} c !{\vline width 2pt} c !{\vline width 2pt} c}
				\; & \; & \; & \; & \; \\ 
				\; & \; & \; & \; & \;\\
				\; & \; & \; & \;& \; \\
				\; & \; & \; & \;& \; \\
			\end{array}
		\end{bmatrix}
		\begin{bmatrix}
		* \\
		* \\
		* \\
		* 
		\end{bmatrix} 
		= * 
		\begin{bmatrix}
		\begin{array} {c!{\vline width 2pt} l}
		 \; &\\
		 \; &\\
		 \; &\\
		 \; &
		\end{array}
		\end{bmatrix} 
		 + * 
		\begin{bmatrix}
		\begin{array} {c!{\vline width 2pt} l}
		 \; &\\
		 \; &\\
		 \; &\\
		 \; &
		\end{array}
		\end{bmatrix} 
		 + *
		\begin{bmatrix}
		\begin{array} {c!{\vline width 2pt} l}
		 \; &\\
		 \; &\\
		 \; &\\
		 \; &
		\end{array}
		\end{bmatrix} 
		 + * 
		\begin{bmatrix}
		\begin{array} {c!{\vline width 2pt} l}
		 \; &\\
		 \; &\\
		 \; &\\
		 \; &
		\end{array}
		\end{bmatrix} 
	\]
	
In words, multiplying a matrix by a column vector means that each entry of the column vector scales the corresponding column in the matrix by that entry. That sounds weird, the first entry of the column vector scales the first column of $A$, the second entry in the column vector scales the second column of $A$, and continues. We can think about the column vector rotating 90 degrees and imposing into the matrix $A$ and then collapsing horizontally to create the resulting column vector $Ax = b$.

On the other side, if we multiply $A$ by a row vector, we are multiplying $A$ on the left by the row vector and similarly we have 
\begin{align*}
	\begin{bmatrix} 
		* & * & * & * 
	\end{bmatrix}
	\begin{bmatrix}
	\begin{array}{c c c c c c}
	& & & & & \\ \Xhline{2pt}
	& & & & & \\ \Xhline{2pt}
	& & & & & \\ \Xhline{2pt}
	& & & & &\\ \Xhline{2pt}
	& & & & &
	\end{array}
	\end{bmatrix}  = \;\;\;\;\;&* \begin{bmatrix}\begin{array}{c c c c c c}
	& & & & & \\ \Xhline{2pt}
	& & & & & \\ 
	\end{array}
	\end{bmatrix} \\
 + \;\;& 
	* \begin{bmatrix}\begin{array}{c c c c c c}
	& & & & & \\ \Xhline{2pt}
	& & & & & \\ 
	\end{array}
	\end{bmatrix} \\\\
 + \;\;& 
	* \begin{bmatrix}\begin{array}{c c c c c c}
	& & & & & \\ \Xhline{2pt}
	& & & & & \\ 
	\end{array}
	\end{bmatrix} \\\\
+\;\; & 
	* \begin{bmatrix}\begin{array}{c c c c c c}
	& & & & & \\ \Xhline{2pt}
	& & & & & \\ 
	\end{array}
	\end{bmatrix}\\
	\cline{2-2} 
	= &\;\;\; \text{the row vector}
\end{align*}
So, what this does is that the row vector is rotating 90 degrees and then imposing itself on the rows of $A$ to scale each row by the entries in the row vector. Then, we add together by collapsing downwards (vertically) to compute the resulting row vector of $x^TA = b^T$.

So, when we are working with diagonal matrices we do the same thing because all entries off the diagonal are 0 the resulting computation of $A$ times a diagonal matrix on the left will scale the rows by each diagonal entry whereas if we multiply $A$ by the diagonal matrix on the right we will scale each column of $A$ by the corresponding diagonal entry. 

Super super cool stuff.\\

\hrule\vspace{2pt}
\hrule
\break 

\begin{problem}{\#2, Summer 2021} Let $A$ be an $n\times n$ real matrix. Suppose $\tr{AX} = 0$ for any $n\times n$ real matrix $X$ with $\tr{X} = 0$. Prove that $A = \lambda I_{n\times n}$ for some $\lambda \in \R$.
\end{problem}

\textbf{Solution:} 

Let $i = 2,\dots, n$ be arbitrary and define $E$ to be the matrix where $E_{11} = 1$ and $E_{ii} = -1$ and otherwise all entries of $E$ are equal to 0. Then, 
	\[ \tr{E} = \sum_{k=1}^n E_{kk} = 1 + 0 + \cdots + 0 + (-1) + 0 + \cdots 0 = 0.\]
We compute,
	\begin{align*} 
		AE & =  
		\begin{bmatrix}
			& \\\\
			& \\\\
			\;\;A_1 & A_2 & \cdots & A_i & \cdots & A_n
			& \\\\\\
			&
		\end{bmatrix}	 
		\begin{bmatrix}
			\begin{array}{cccccccc}
 				1 &   &   &   &   &   &   &   \\
 				  & 0 &   &   &   & \mathbf 0  &   &   \\
 				  &   & \ddots &   &   &   &   &   \\
				  &   &   & 0 &   &   &   &   \\
 				  &   &   &   & -1 &   &   &   \\
 				  &   &   &   &   & 0 &   &   \\
 				  &   &   \mathbf 0&   &   &   & \ddots &   \\
 				  &   &   &   &   &   &   & 0 \\
			\end{array}
		   \end{bmatrix}\\
		   & \quad \quad \quad = \begin{bmatrix} 
		   	& \\\\
			& \\
		   	A_1 & \mathbf 0 & \cdots & \mathbf 0  & -A_i & \mathbf 0 & \cdots & \mathbf 0\\
			& \\\\
			&
			\end{bmatrix}\\
		 & \quad \quad \quad = \begin{bmatrix} 
		 	\begin{array}{cccccccc}
 				A_{11} &   &   &   &   &   &   &   \\
 				  & 0 &   &   &   & \Box  &   &   \\
 				  &   & \ddots &   &   &   &   &   \\
				  &   &   & 0 &   &   &   &   \\
 				  &   &   &   & -A_{ii} &   &   &   \\
 				  &   &   &   &   & 0 &   &   \\
 				  &   &   \Box &   &   &   & \ddots &   \\
 				  &   &   &   &   &   &   & 0 \\
			\end{array}
		 \end{bmatrix}
	\end{align*}
where the boxes denote the entries above and below the diagonal and $A_k$ denotes the $k$-th column of $A$. Since $\tr{E} = 0$, $\tr{AE} = 0$ which implies that $A_{11} - A_{ii} = 0$, i.e., $A_{11} = A_{ii}$ for all $i = 2, \dots, n$. Let $\lambda \in \R$. Then, the entries along the diagonal of $A$ are all equal to $\lambda$. 
	
Now, let $E$ be the matrix with $E_{kk} = 0$ for all $k = 1, \dots, n$ and $E_{ij} = 1$ for some $i\neq j$ and all other entries are equal to 0. In other words, the $j$-th column of $E$ is a column vector of all zeros except in the $i$-th row for $i\neq j$. We compute,
	\begin{align*} 
		AE & =  
		\begin{bmatrix}
			& \\\\
			& \\\\
			A_1 & \cdots & A_i & \cdots & A_n
			& \\\\\\
			&
		\end{bmatrix}
		\begin{bmatrix}
			& \\\\
			& \\\\
			\;\;\mathbf 0 & \cdots & \mathbf 0& E_j & \mathbf 0 \cdots & \mathbf 0
			& \\\\\\
			&
		\end{bmatrix}\\
		& \quad =  \begin{bmatrix}
			& \\\\
			& \\\\
			\;\;\mathbf 0 & \cdots & \mathbf 0& AE_j & \mathbf 0 \cdots & \mathbf 0
			& \\\\\\
			&
		\end{bmatrix}
	\end{align*}
and compute $AE_j$ (the matrix $A$ times the column vector $E_j$) 
	\[AE_j = 0\cdot A_1 + \cdots + 0 \cdot A_{i-1} + 1\cdot A_i + 0\cdot A_{i+1} + \cdots + 0\cdot A_n = A_i\]
which means that the $j$-th column of $AE$ equals the $i$-th column of $A$, since $E_j$ is the column vector with 1 in the $i$-th slot for $i\neq j$. That is, 
	\[A_{1i} = (AE)_{1j}, \dots, A_{ij} = (AE)_{jj}, \dots, \text{ and } A_{1n} = (AE)_{nj}.\]
	 By the assumption that $\tr{AE} = 0$ for any matrix with $\tr{E} = 0$, we conclude that $A_{ij} = 0$. Since $i$ and $j$ with $i\neq j$ were arbitrary in the above calculation, $A_{ij} = 0$ for all $i\neq j$. 

Therefore, $A = \lambda I_{n\times n}$ for some $\lambda \in \R$.\\

\hrule

\textbf{Notes:} Welp. I did that. Time to take a break.\\

\hrule \vspace{2pt}
\hrule


\end{document}